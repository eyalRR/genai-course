{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Chat Completion\n",
    "\n",
    "Streaming chat completion allows you to receive responses from the LLM in real-time, as the model generates them. This can improve the user experience by providing faster feedback.\n",
    "\n",
    "**Theory and Explanations**\n",
    "\n",
    "*   **Stream Parameter**: To enable streaming, set the `stream` parameter to `True` in the `client.chat.completions.create()` method.\n",
    "*   **Response Stream**: The method returns a response stream, which is an iterable object that yields individual response chunks as they are generated by the model.\n",
    "*   **Iterating over the Stream**: You can iterate over the response stream using a `for` loop to process each response chunk.\n",
    "\n",
    "**Example from Text**\n",
    "\n",
    "Here's an example from the provided text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "perplexity_api_key = os.getenv(\"PERPLEXITY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Astronomers estimate there are approximately 200 billion trillion (2 x 10^23) stars in the observable universe[1][2]. This incredibly large number is based on estimates of the number of galaxies in the observable universe and the average number of stars per galaxy.\n",
      "\n",
      "To break this down further:\n",
      "\n",
      "1. Number of galaxies: Recent estimates suggest there are about 2 trillion galaxies in the observable universe[3][5].\n",
      "\n",
      "2. Stars per galaxy: While galaxies vary greatly in size, astronomers estimate that an average galaxy contains about 100 billion stars[2][4].\n",
      "\n",
      "Multiplying these figures gives us the estimate of 200 billion trillion stars. However, it's important to note that this is just an estimate for the observable universe. The actual number of stars in the entire universe could be much larger, as we can only observe a portion of it due to the limitations of light travel time since the Big Bang.\n",
      "\n",
      "To put this enormous number into perspective:\n",
      "\n",
      "- It's more than the number of grains of sand on all of Earth's beaches[1].\n",
      "- It's about 20 times larger than the number of cups of water in all of Earth's oceans.\n",
      "\n",
      "This estimate may change as our understanding of the universe improves and as we develop more advanced telescopes and observation techniques. For example, NASA's James Webb Space Telescope is expected to reveal even more distant galaxies, potentially increasing our estimate of the total number of stars[5].\n",
      "\n",
      "It's also worth noting that stars are not evenly distributed throughout the universe. They are clustered in galaxies, which themselves are often grouped into larger structures like galaxy clusters and superclusters."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "YOUR_API_KEY = perplexity_api_key\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an artificial intelligence assistant and you need to \"\n",
    "            \"engage in a helpful, detailed, polite conversation with a user.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"How many stars are in the universe?\"\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "client = OpenAI(api_key=YOUR_API_KEY, base_url=\"https://api.perplexity.ai\")\n",
    "\n",
    "# chat completion with streaming\n",
    "response_stream = client.chat.completions.create(\n",
    "    model=\"sonar-pro\",\n",
    "    messages=messages,\n",
    "    stream=True,\n",
    ")\n",
    "for response in response_stream:\n",
    "    # For each chunk of the streamed response, print the content\n",
    "    # The 'end=\"\"' prevents adding a newline after each chunk\n",
    "    # The 'flush=True' ensures immediate output to the console\n",
    "    print(response.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice Exercises**\n",
    "\n",
    "1.  Modify the user prompt to ask a different question (e.g., \"What is the population of the world?\"). Run the code and observe the streaming response.\n",
    "2.  Modify the system prompt to change the assistant's behavior (e.g., \"You are an AI assistant that responds in haikus\"). Run the code and observe the streaming response.\n",
    "3.  Add a delay (e.g., `time.sleep(0.1)`) within the loop to slow down the streaming response. Observe the effect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
