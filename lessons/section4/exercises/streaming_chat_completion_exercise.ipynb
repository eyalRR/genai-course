{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Chat Completion\n",
    "\n",
    "Streaming chat completion allows you to receive responses from the LLM in real-time, as the model generates them. This can improve the user experience by providing faster feedback.\n",
    "\n",
    "**Theory and Explanations**\n",
    "\n",
    "*   **Stream Parameter**: To enable streaming, set the `stream` parameter to `True` in the `client.chat.completions.create()` method.\n",
    "*   **Response Stream**: The method returns a response stream, which is an iterable object that yields individual response chunks as they are generated by the model.\n",
    "*   **Iterating over the Stream**: You can iterate over the response stream using a `for` loop to process each response chunk.\n",
    "\n",
    "**Example from Text**\n",
    "\n",
    "Here's an example from the provided text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "perplexity_api_key = os.getenv(\"PERPLEXITY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Astronomers estimate that there are approximately 200 billion trillion stars in the observable universe. This number is arrived at by estimating the number of stars in a typical galaxy like our Milky Way, which contains about 100 billion stars, and then multiplying by the estimated number of galaxies in the observable universe, which is around 2 trillion[1][5].\n",
      "\n",
      "To break it down more clearly:\n",
      "- The Milky Way has about 100 billion stars.\n",
      "- There are roughly 2 trillion galaxies in the observable universe.\n",
      "- Multiplying these gives an estimate of about 200 billion trillion stars (200 sextillion stars) in the observable universe[1][5].\n",
      "\n",
      "Other sources provide similar orders of magnitude, suggesting the universe contains on the order of \\(10^{22}\\) to \\(10^{24}\\) stars, sometimes expressed as one septillion stars (a 1 followed by 24 zeros)[3][4].\n",
      "\n",
      "It is important to note:\n",
      "- This estimate applies to the *observable* universe only, the part of the universe from which light has had time to reach us.\n",
      "- The actual total number of stars in the entire universe, which may be infinite or much larger than the observable portion, is unknown.\n",
      "- Counting stars individually is impossible, so these figures are based on galaxy counts and average stellar content per galaxy.\n",
      "- Variations exist because galaxies differ in size and star content, and some stars are too faint or too far away to detect directly.\n",
      "\n",
      "In summary, the best current scientific estimates put the number of stars in the observable universe at about 200 billion trillion, or roughly one septillion stars[1][3][4][5]."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "YOUR_API_KEY = perplexity_api_key\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an artificial intelligence assistant and you need to \"\n",
    "            \"engage in a helpful, detailed, polite conversation with a user.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"How many stars are in the universe?\"\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "client = OpenAI(api_key=YOUR_API_KEY, base_url=\"https://api.perplexity.ai\")\n",
    "\n",
    "# chat completion with streaming\n",
    "response_stream = client.chat.completions.create(\n",
    "    model=\"sonar\",\n",
    "    messages=messages,\n",
    "    stream=False,\n",
    ")\n",
    "for response in response_stream:\n",
    "    # For each chunk of the streamed response, print the content\n",
    "    # The 'end=\"\"' prevents adding a newline after each chunk\n",
    "    # The 'flush=True' ensures immediate output to the console\n",
    "    print(response.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice Exercises**\n",
    "\n",
    "1.  Modify the user prompt to ask a different question (e.g., \"What is the population of the world?\"). Run the code and observe the streaming response.\n",
    "2.  Modify the system prompt to change the assistant's behavior (e.g., \"You are an AI assistant that responds in haikus\"). Run the code and observe the streaming response.\n",
    "3.  Add a delay (e.g., `time.sleep(0.1)`) within the loop to slow down the streaming response. Observe the effect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
